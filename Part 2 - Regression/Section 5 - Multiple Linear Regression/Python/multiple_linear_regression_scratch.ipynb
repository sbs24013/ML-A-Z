{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CazISR8X_HUG"},"source":["# Multiple Linear Regression"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pOyqYHTk_Q57"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vgC61-ah_WIz"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["dataset = pd.read_csv('50_Startups.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VadrvE7s_lS9"},"source":["## Encoding categorical data"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","\n","ct = ColumnTransformer(\n","  transformers=[\n","      ('encoder', OneHotEncoder(), [3])\n","  ], remainder='passthrough')\n","\n","X = np.array(ct.fit_transform(X))"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n"," [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n"," [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n"," [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n"," [0.0 1.0 0.0 142107.34 91391.77 366168.42]]\n"]}],"source":["print(X[:5])\n","# 인코딩 된 더미 값이 가장 먼저 나옴\n","# Multiple Linear Regression - Don't need to apply \"Feature Scaling\"\n","# 각 특성에 적용되는 가중치가 스케일링 역할을 한다"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WemVnqgeA70k"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k-McZVsQBINc"},"source":["## Training the Multiple Linear Regression model on the Training set\n","\n","Dummy variable trap?\n","- No need to worry about\n","- 클래스가 알아서 처리\n","\n","Backward eliminations?(select best features)\n","- No\n","- 클래스가 처리\n","- 클래스가 특성의 중요도를 알아서 처리"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"],"text/plain":["LinearRegression()"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.linear_model import LinearRegression\n","\n","regressor = LinearRegression()\n","# train\n","regressor.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xNkXL1YQBiBT"},"source":["## Predicting the Test set results"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[103015.2  103282.38]\n"," [132582.28 144259.4 ]\n"," [132447.74 146121.95]\n"," [ 71976.1   77798.83]\n"," [178537.48 191050.39]\n"," [116161.24 105008.31]\n"," [ 67851.69  81229.06]\n"," [ 98791.73  97483.56]\n"," [113969.44 110352.25]\n"," [167921.07 166187.94]]\n"]}],"source":["# test_y\n","# pred_y\n","\n","y_pred = regressor.predict(X_test)\n","np.set_printoptions(precision=2)\n","# 2 decimals\n","\n","print(np.concatenate(\n","  (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1))\n","  ,1\n","))\n","# np.concatenate((array1, array2), horizontal)\n","\n","# y_pred.reshape(len(y_pred),1): 1개의 아이템을 가진 10개의 배열로 바꿈\n","# array.reshape(A,B): A개의 B크기의 배열로 형태를 변환 A x B == len(array)\n","\n","# print(y_pred): 10개의 아이템을 가진 1차원 배열\n","# print(y_pred.reshape(5,2)): 2개의 아이템으로 구성된 배열을 5개 가진 3차원 배열"]},{"cell_type":"markdown","metadata":{},"source":["# Implementing this Backward Elimination technique"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (3148461200.py, line 44)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[54], line 44\u001b[1;36m\u001b[0m\n\u001b[1;33m    regressor_OLS.summary()X_opt = X[:, [0, 1, 3, 4, 5]]\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}],"source":["# Multiple Linear Regression\n"," \n","# Importing the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"," \n","# Importing the dataset\n","dataset = pd.read_csv('50_Startups.csv')\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values\n","print(X)\n"," \n","# Encoding categorical data\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n","X = np.array(ct.fit_transform(X))\n","print(X)\n"," \n","# Avoiding the Dummy Variable Trap\n","X = X[:, 1:]\n"," \n","# Splitting the dataset into the Training set and Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"," \n","# Training the Multiple Linear Regression model on the Training set\n","from sklearn.linear_model import LinearRegression\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n"," \n","# Predicting the Test set results\n","y_pred = regressor.predict(X_test)\n","np.set_printoptions(precision=2)\n","print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n"," \n","# Building the optimal model using Backward Elimination\n","import statsmodels.api as sm\n","X = np.append(arr = np.ones((50, 1)).astype(int), values = X, axis = 1)\n","X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n","X_opt = X_opt.astype(np.float64)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","regressor_OLS.summary()\n","X_opt = X[:, [0, 1, 3, 4, 5]]\n","X_opt = X_opt.astype(np.float64)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","regressor_OLS.summary()\n","X_opt = X[:, [0, 3, 4, 5]]\n","X_opt = X_opt.astype(np.float64)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","regressor_OLS.summary()\n","X_opt = X[:, [0, 3, 5]]\n","X_opt = X_opt.astype(np.float64)\n","regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","regressor_OLS.summary()\n","X_opt = X[:, [0, 3]]\n","X_opt = X_opt.astype(np.float64)regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","regressor_OLS.summary()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPhYhte6t7H4wEK4xPpDWT7","name":"Multiple Linear Regression","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
