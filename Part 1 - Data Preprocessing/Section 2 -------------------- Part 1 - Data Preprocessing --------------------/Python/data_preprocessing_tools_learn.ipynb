{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np # array\n",
        "import matplotlib.pyplot as plt # charts\n",
        "import pandas as pd # preprocess "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"Data.csv\")\n",
        "\n",
        "# 0 1 2 컬럼\n",
        "# iloc[row_index_range, col_index_range]\n",
        "# iloc[:,:-1]: all the rows & except last col\n",
        "# -1: last column\n",
        "# A:B include lowerbound (A) except upperbound (B)\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "\n",
        "# dependent variables vector\n",
        "# last col\n",
        "# 모든 행, 마지막 열\n",
        "y = dataset.iloc[:, -1].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1% remove data\n",
        "\n",
        "# replace missing data to avg value\n",
        "# SimpleImputer \n",
        "# Univariate imputer for completing missing values with simple strategies.\n",
        "# Replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column, or using a constant value.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3]) # Age, Salary # Select all the numberic columns\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3]) # Return updated dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data\n",
        "\n",
        "avoid mis interpretation\n",
        "\n",
        "one hot Encoding\n",
        "\n",
        "country column -> 3 colums (3 different classes) / 5 cols (5 diff classes)\n",
        "\n",
        "France  - 1 0 0 \n",
        "Spain   - 0 1 0 \n",
        "Germany - 0 0 1\n",
        "\n",
        "Avoid numeric order\n",
        "\n",
        "Purchased - 1 0 - Binary outcome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coding Exercise 3: Encoding Categorical Data for Machine Learning\n",
        "1: Import required libraries - Pandas, Numpy, and required classes for this task - ColumnTransformer, OneHotEncoder, LabelEncoder.\n",
        "\n",
        "2: Start by loading the Titanic dataset into a pandas data frame. This can be done using the pd.read_csv function. The dataset's name is 'titanic.csv'.\n",
        "\n",
        "3: Identify the categorical features in your dataset that need to be encoded. You can store these feature names in a list for easy access later.\n",
        "\n",
        "4: To apply OneHotEncoding to these categorical features, create an instance of the ColumnTransformer class. Make sure to pass the OneHotEncoder() as an argument along with the list of categorical features.\n",
        "\n",
        "5: Use the fit_transform method on the instance of ColumnTransformer to apply the OneHotEncoding.\n",
        "\n",
        "6: The output of the fit_transform method should be converted into a NumPy array for further use.\n",
        "\n",
        "7: The 'Survived' column in your dataset is the dependent variable. This is a binary categorical variable that should be encoded using LabelEncoder.\n",
        "\n",
        "8: Print the updated matrix of features and the dependent variable vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Importing the necessary libraries\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "# # Load the dataset\n",
        "# df = pd.read_csv('titanic.csv')\n",
        "\n",
        "# # Identify the categorical data\n",
        "# categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
        "\n",
        "# # Implement an instance of the ColumnTransformer class\n",
        "# ct = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('encoder', OneHotEncoder(), categorical_features)\n",
        "#     ], remainder='passthrough')\n",
        "\n",
        "# # Apply the fit_transform method on the instance of ColumnTransformer\n",
        "# X = ct.fit_transform(df)\n",
        "\n",
        "# # Convert the output into a NumPy array\n",
        "# X = np.array(X)\n",
        "\n",
        "# # Use LabelEncoder to encode binary categorical data\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(df['Survived'])\n",
        "\n",
        "# # Print the updated matrix of features and the dependent variable vector\n",
        "# print(\"Updated matrix of features: \\n\", X)\n",
        "# print(\"Updated dependent variable vector: \\n\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set\n",
        "\n",
        "훈련세트와 테스트세트 분리는 훈련 세트만 사용하여 모델링을 하고 모델을 테스트 세트를 사용하여 평가하는 것이다.\n",
        "\n",
        "반면 스케일링은 의존 변수(features) 끼리 독립 변수에 미치는 영향력을 균등하게 하기 위해 사이즈를 조정하는 것이다.\n",
        "\n",
        "만약, 스케일링은 한 후, 스플릿팅을 하는 경우, 테스트 세트의 정보가 스케일링에 반영되어, 테스트 세트가 순수하게 분리되지 않게 된다.\n",
        "\n",
        "그렇기 때문에 반드시, 훈련세트와 테스트세트를 분리한 후, 스케일링을 해야한되다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from \"package.module\" import \"method\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "# X: Independent\n",
        "# y: Dependent\n",
        "# train: 80%\n",
        "# test: 20%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Import necessary libraries\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Load the Iris dataset using pd.read_csv\n",
        "# iris_df = pd.read_csv('iris.csv')\n",
        "\n",
        "# # Separate features and target\n",
        "# X = iris_df.drop('target', axis=1)  # Assuming 'target' is the column name       for the target variable\n",
        "# y = iris_df['target']\n",
        "\n",
        "# # Split the dataset into an 80-20 training-test set\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Apply StandardScaler to scale the feature variables\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# # Print scaled training and test sets\n",
        "# print(\"Scaled Training Set:\")\n",
        "# print(X_train)\n",
        "# print(\"\\nScaled Test Set:\")\n",
        "# print(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "모든 특성이 같은 크기를 같도록 하는 것\n",
        "한 특성으로 인해 다른 특성이 무시되는 것을 방지\n",
        "\n",
        "Standardisation\n",
        "- (X - mean(x)) / standard deviation(x)\n",
        "- (X - mu) / sigma\n",
        "- between -3 and +3\n",
        "- Always work\n",
        "\n",
        "Normalisation\n",
        "- (X - min(x)) / (max(x) - min(x))\n",
        "- between 0 and 1\n",
        "- When most of features follow normalisation distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# create instance of class\n",
        "sc = StandardScaler()\n",
        "\n",
        "# 더미 값을 스케일 해야하는 가? NO\n",
        "# 스케일링을 하는 경우 인코딩의 의미를 잃어버린다. \n",
        "# \"숫자형\" 컬럼에만 스케일링을 적용 (3, 4 컬럼)\n",
        "\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "\n",
        "# 테스트 세트는 트레이닝 세트를 기반으로 스케일링이 되어야 한다\n",
        "# 트레이닝 세트에서 사용된 스케일러를 그대로 사용\n",
        "X_test[:, 3:] = sc.fit_transform(X_test[:, 3:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.1915918438457856 -1.0781259408412427]\n",
            " [0.0 1.0 0.0 -0.014117293757057902 -0.07013167641635401]\n",
            " [1.0 0.0 0.0 0.5667085065333239 0.6335624327104546]\n",
            " [0.0 0.0 1.0 -0.3045301939022488 -0.30786617274297895]\n",
            " [0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]\n",
            " [1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]\n",
            " [0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]\n",
            " [1.0 0.0 0.0 -0.7401495441200352 -0.5646194287757336]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.0 -1.0]\n",
            " [1.0 0.0 0.0 1.0 1.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
